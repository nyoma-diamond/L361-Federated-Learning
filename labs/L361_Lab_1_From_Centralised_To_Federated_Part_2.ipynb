{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTR0VzOm2B5d"
   },
   "source": [
    "# 0. Marking.\n",
    "\n",
    "***IMPORTANT***: Save a copy of this notebook into your Drive before you start.\n",
    "- Please attempt all the questions marked for your group (Part II ✅ | Part III/MPhil ✅).\n",
    "\n",
    "Please submit a zip file, containing both parts, consiting of of:\n",
    "1. A text file with a publicly visible link to your notebooks in Google Colab or GitHub.\n",
    "2. A downloaded copy (ipynb) of your notebooks or your zipped cloned GitHub repo. You may treat these as a report: we will not be re-executing the code you used to produce the answers unless required.\n",
    "\n",
    "If you find yourself enjoying the material, feel free to attempt more! Provide your answers in a new cell below the question cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1P9epXi-IZ0"
   },
   "source": [
    "### Globals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTM8comw-IZ2"
   },
   "outputs": [],
   "source": [
    "# The simulation component of the flower framework uses RAY under the hood.\n",
    "# `pip` could produce some errors. Do not worry about them.\n",
    "# The execution has been verified; it's working anyway.\n",
    "! pip install --quiet --upgrade \"pip\"\n",
    "! pip install --quiet git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching torch torchvision matplotlib gdown tqdm ray==\"2.6.3\" seaborn\n",
    "# The following is just needed to show the folder tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9YC7eFA-IZ1"
   },
   "outputs": [],
   "source": [
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zl2hUa1-IZ2"
   },
   "source": [
    "### Imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7xM_SyD-IZ4"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numbers\n",
    "import random\n",
    "from collections import OrderedDict, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import flwr as fl\n",
    "import gdown\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from flwr.common.parameter import ndarrays_to_parameters\n",
    "from flwr.common.typing import NDArrays, Parameters, Scalar\n",
    "from flwr.server import ServerConfig, History\n",
    "from flwr.client import Client\n",
    "from flwr.server.strategy import FedAvgM as FedAvg, Strategy\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from enum import IntEnum\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Add new seeds here for easy autocomplete\n",
    "class Seeds(IntEnum):\n",
    "    DEFAULT = 1337\n",
    "\n",
    "\n",
    "np.random.seed(Seeds.DEFAULT)\n",
    "random.seed(Seeds.DEFAULT)\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int32 | np.int64):\n",
    "        return int(o)\n",
    "    if isinstance(o, np.float32 | np.float64):\n",
    "        return float(o)\n",
    "    raise TypeError\n",
    "\n",
    "\n",
    "def fit_client_seeded(client, params, conf, seed=Seeds.DEFAULT, **kwargs):\n",
    "    \"\"\"Wrapper to always seed client training.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    return client.fit(params, conf, **kwargs)\n",
    "\n",
    "\n",
    "PathType = Optional[Path | str]\n",
    "\n",
    "\n",
    "def get_device() -> str:\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = \"mps\"\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "341p1c5o-IZ4"
   },
   "outputs": [],
   "source": [
    "home_dir = content if (content := Path(\"/content\")).exists() else Path.cwd()\n",
    "dataset_dir: Path = home_dir / \"femnist\"\n",
    "data_dir: Path = dataset_dir / \"data\"\n",
    "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
    "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
    "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n",
    "\n",
    "#  Download compressed dataset\n",
    "if not (home_dir / \"femnist.tar.gz\").exists():\n",
    "    id = \"1-CI6-QoEmGiInV23-n_l6Yd8QGWerw8-\"\n",
    "    gdown.download(\n",
    "        f\"https://drive.google.com/uc?export=download&confirm=pbef&id={id}\",\n",
    "        str(home_dir / \"femnist.tar.gz\"),\n",
    "    )\n",
    "\n",
    "# Decompress dataset\n",
    "if not dataset_dir.exists():\n",
    "    !tar -xf {str(home_dir)}/femnist.tar.gz -C {str(home_dir)} 2> /dev/null\n",
    "    print(f\"Dataset extracted in {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82ktoKJ4-IZ5"
   },
   "outputs": [],
   "source": [
    "if not (home_dir / \"common\").exists():\n",
    "    ! git clone \"https://github.com/camlsys/L361-Federated-Learning.git\" temp_repo\n",
    "\n",
    "    # Copy the folder to the current directory\n",
    "    ! cp -r \"temp_repo/labs/common\" {home_dir}\n",
    "\n",
    "    # Delete the cloned repository\n",
    "    ! rm -rf temp_repo\n",
    "\n",
    "(home_dir / \"__init__.py\").open(mode=\"a+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bLMKg-y-IZ5"
   },
   "outputs": [],
   "source": [
    "from common.client_utils import (\n",
    "    Net,\n",
    "    load_FEMNIST_dataset,\n",
    "    get_network_generator_cnn as get_network_generator,\n",
    "    train_FEMNIST,\n",
    "    test_FEMNIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Q6qaJbjac5"
   },
   "source": [
    "# 5. Building a Flower FL client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUEPLGXCxhDe"
   },
   "source": [
    "Our first client abstraction shall be as simple as possible and will require adjustment to match the structure that the flower framework expects. However, it shall be conceptually identical and require only light API changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh6ebOepFBUC"
   },
   "source": [
    "Moving from centralised ML to server-client FL requires us to provide a means of communication between the respective server and clients. The Flower Framework is ML-framework agnostic and allows various means of transmitting model parameters in the federated network. Since you may have limited resources in these labs, we will only tangentially follow the Flower framework while keeping the computational requirements to a minimum.\n",
    "\n",
    "The simplest and most common encoding for models is the mere transmission of model parameters as NumPy arrays instead of the stateful PyTorch models. The following functions allow for seamless conversions between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh2wjiNsVTZI"
   },
   "outputs": [],
   "source": [
    "def set_model_parameters(net: Module, parameters: NDArrays) -> Module:\n",
    "    \"\"\"Function to put a set of parameters into the model object.\n",
    "\n",
    "    Args:\n",
    "        net (Module): model object.\n",
    "        parameters (NDArrays): set of parameters to put into the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Module: updated model object.\n",
    "    \"\"\"\n",
    "    weights = parameters\n",
    "    params_dict = zip(net.state_dict().keys(), weights, strict=False)\n",
    "    state_dict = OrderedDict({k: torch.from_numpy(np.copy(v)) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_model_parameters(net: Module) -> NDArrays:\n",
    "    \"\"\"Function to get the current model parameters as NDArrays.\n",
    "\n",
    "    Args:\n",
    "        net (Module): current model object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        NDArrays: set of parameters from the current model.\n",
    "    \"\"\"\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHJGcNa7FBUD"
   },
   "source": [
    "With them in place, making the client abstraction compatible with Flower requires only a bit of boilerplate such as allowing NumPy arrays to be received and sent instead of PyTorch models. To achieve this, we provide a model generator capable of creating a network and using the received parameters.\n",
    "\n",
    "To keep client objects light in the memory when not used by the Flower FL simulator, the model generator is only called as needed for either `fit` or `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCqhfpnJobOZ"
   },
   "outputs": [],
   "source": [
    "class FlowerRayClient(fl.client.NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        partition_dir: Path,\n",
    "        model_generator: Callable[[], Module],\n",
    "    ) -> None:\n",
    "        \"\"\"Function to initialise the client with its unique id\n",
    "        and the directory from which it can load its data.\n",
    "\n",
    "        Args:\n",
    "            cid (int): Unique client id for a client used to map it to its data partition\n",
    "            partition_dir (Path): The directory containing data for each client/client id\n",
    "            model_generator (Callable[[], Module]): The model generator function\n",
    "        \"\"\"\n",
    "        self.cid = cid\n",
    "        print(\"cid: \", self.cid)\n",
    "        self.partition_dir = partition_dir\n",
    "        self.device = str(\n",
    "            torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.model_generator: Callable[[], Module] = model_generator\n",
    "        self.properties: Dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\"}\n",
    "\n",
    "    def set_parameters(self, parameters: NDArrays) -> Module:\n",
    "        \"\"\"Loads weights inside the network.\n",
    "\n",
    "        Args:\n",
    "            parameters (NDArrays): set of weights to be loaded.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            [Module]: Network with new set of weights.\n",
    "        \"\"\"\n",
    "        net = self.model_generator()\n",
    "        return set_model_parameters(net, parameters)\n",
    "\n",
    "    def get_parameters(self, config: Dict[int, Scalar]) -> NDArrays:\n",
    "        \"\"\"Returns weights from a given model. If no model is passed, then a\n",
    "        local model is created. This can be used to initialise a model in the\n",
    "        server.\n",
    "        The config param is not used but is mandatory in Flower.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[int, Scalar]): dictionary containing configuration info.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            NDArrays: weights from the model.\n",
    "        \"\"\"\n",
    "        net = self.model_generator()\n",
    "        return get_model_parameters(net)\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Tuple[NDArrays, int, dict]:\n",
    "        \"\"\"Function which receives and trains a model on the local client data using parameters from the config dict\n",
    "\n",
    "        Args:\n",
    "            net (NDArrays): Pytorch model parameters\n",
    "            config (Dict[str, Scalar]): Dictionary describing the training parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple[NDArrays, int, dict]: Returns the updated model, the size of the local dataset and other metrics\n",
    "        \"\"\"\n",
    "        # Only create model right before training/testing\n",
    "        # To lower memory usage when idle\n",
    "        net = self.set_parameters(parameters)\n",
    "        net.to(self.device)\n",
    "\n",
    "        train_loader: DataLoader = self._create_data_loader(config, name=\"train\")\n",
    "        train_loss = self._train(net, train_loader=train_loader, config=config)\n",
    "        return get_model_parameters(net), len(train_loader), {\"train_loss\": train_loss}\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, int, dict]:\n",
    "        \"\"\"Function which receives and tests a model on the local client data using parameters from the config dict\n",
    "\n",
    "        Args:\n",
    "            net (NDArrays): Pytorch model parameters\n",
    "            config (Dict[str, Scalar]): Dictionary describing the testing parameters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple[float, int, dict]: Returns the loss accumulate during testing, the size of the local dataset and other metrics such as accuracy\n",
    "        \"\"\"\n",
    "        net = self.set_parameters(parameters)\n",
    "        net.to(self.device)\n",
    "\n",
    "        test_loader: DataLoader = self._create_data_loader(config, name=\"test\")\n",
    "        loss, accuracy = self._test(net, test_loader=test_loader, config=config)\n",
    "        return loss, len(test_loader), {\"local_accuracy\": accuracy}\n",
    "\n",
    "    def _create_data_loader(self, config: Dict[str, Scalar], name: str) -> DataLoader:\n",
    "        \"\"\"Creates the data loader using the specified config parameters\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Scalar]): Dictionary containing dataloader and dataset parameters\n",
    "            mode (str): Load the training or testing set for the client\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            DataLoader: A pytorch dataloader iterable for training/testing\n",
    "        \"\"\"\n",
    "        batch_size = int(config[\"batch_size\"])\n",
    "        num_workers = int(config[\"num_workers\"])\n",
    "        dataset = self._load_dataset(name)\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=(name == \"train\"),\n",
    "        )\n",
    "\n",
    "    def _load_dataset(self, name) -> Dataset:\n",
    "        full_file: Path = self.partition_dir / str(self.cid)\n",
    "        return load_FEMNIST_dataset(\n",
    "            mapping=full_file,\n",
    "            name=name,\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "\n",
    "    def _train(\n",
    "        self, net: Module, train_loader: DataLoader, config: Dict[str, Scalar]\n",
    "    ) -> float:\n",
    "        return train_FEMNIST(\n",
    "            net=net,\n",
    "            train_loader=train_loader,\n",
    "            epochs=int(config[\"epochs\"]),\n",
    "            device=self.device,\n",
    "            optimizer=torch.optim.AdamW(\n",
    "                net.parameters(),\n",
    "                lr=float(config[\"client_learning_rate\"]),\n",
    "                weight_decay=float(config[\"weight_decay\"]),\n",
    "            ),\n",
    "            criterion=torch.nn.CrossEntropyLoss(),\n",
    "            max_batches=config[\"max_batches\"],\n",
    "        )\n",
    "\n",
    "    def _test(\n",
    "        self, net, test_loader: DataLoader, config: Dict[str, Scalar]\n",
    "    ) -> Tuple[float, float]:\n",
    "        return test_FEMNIST(\n",
    "            net=net,\n",
    "            test_loader=test_loader,\n",
    "            device=self.device,\n",
    "            criterion=torch.nn.CrossEntropyLoss(),\n",
    "            max_batches=config[\"max_batches\"],\n",
    "        )\n",
    "\n",
    "    def get_properties(self, config: Dict[str, Scalar]) -> Dict[str, Scalar]:\n",
    "        \"\"\"Returns properties for this client.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Scalar]): Options to be used for selecting specific\n",
    "            properties.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Dict[str, Scalar]: Returned properties.\n",
    "        \"\"\"\n",
    "        # pylint: disable=unused-argument\n",
    "        return self.properties\n",
    "\n",
    "    def get_train_set_size(self) -> int:\n",
    "        \"\"\"Returns the client train set size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: train set size of the client.\n",
    "        \"\"\"\n",
    "        return len(self._load_dataset(\"train\"))  # type: ignore\n",
    "\n",
    "    def get_test_set_size(self) -> int:\n",
    "        \"\"\"Returns the client test set size.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: test set size of the client.\n",
    "        \"\"\"\n",
    "        return len(self._load_dataset(\"test\"))  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HdLT_IBFBUD"
   },
   "source": [
    "The underlying FL simulator used by Flower is based on [Ray](https://www.ray.io/). It expects each client only to require a client ID for instantiation. Therefore, using the following generator function, we can determine the specific network used for FL together with the FEMNIST partition to which the `cid` refers.\n",
    "\n",
    "While we will not use `Ray` in this lab due to its heavyweight nature, we will keep all code API compatible with the default flower framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mwfh5JZmUcfd"
   },
   "outputs": [],
   "source": [
    "def get_flower_client_generator(\n",
    "    model_generator: Callable[[], Module],\n",
    "    partition_dir: Path,\n",
    "    mapping_fn: Callable[[int], int] | None = None,\n",
    ") -> Callable[[str], FlowerRayClient]:\n",
    "    \"\"\"Wrapper function for the client instance generator.\n",
    "    This provides the client generator with a model generator function.\n",
    "    Also, the partition directory must be passed.\n",
    "    A mapping function could be used for filtering/ordering clients.\n",
    "\n",
    "    Args:\n",
    "        model_generator (Callable[[], Module]): model generator function.\n",
    "        partition_dir (Path): directory containing the partition.\n",
    "        mapping_fn (Optional[Callable[[int], int]]): function mapping sorted/filtered ids to real cid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[str], FlowerRayClient]: client instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> FlowerRayClient:\n",
    "        \"\"\"Creates a single client instance given the client id `cid`.\n",
    "\n",
    "        Args:\n",
    "            cid (str): client id, Flower requires this to be of type str.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            FlowerRayClient: client instance.\n",
    "        \"\"\"\n",
    "        return FlowerRayClient(\n",
    "            cid=mapping_fn(int(cid)) if mapping_fn is not None else int(cid),\n",
    "            partition_dir=partition_dir,\n",
    "            model_generator=model_generator,\n",
    "        )\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4rBKYJqU3bf"
   },
   "source": [
    "To ensure the Flower client behaves the same as our simple demo client, a simple test using the centralised partition we defined earlier should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udMWNQ0xU8VD"
   },
   "outputs": [],
   "source": [
    "network_generator = get_network_generator()\n",
    "seed_net: Net = network_generator()\n",
    "seed_model_params: NDArrays = get_model_parameters(seed_net)\n",
    "\n",
    "centralized_flower_client_generator: Callable[\n",
    "    [int], FlowerRayClient\n",
    "] = get_flower_client_generator(network_generator, centralized_partition)  # type: ignore\n",
    "centralized_flower_client = centralized_flower_client_generator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJima3hoFBUD"
   },
   "outputs": [],
   "source": [
    "centralized_train_config: Dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "test_config: Dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "# Train parameters on the centralised dataset\n",
    "trained_params, num_examples, train_metrics = fit_client_seeded(\n",
    "    centralized_flower_client, params=seed_model_params, conf=centralized_train_config\n",
    ")\n",
    "print(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdzCbOf0wOWC"
   },
   "outputs": [],
   "source": [
    "# Test trained parameters on the centralised dataset\n",
    "loss, num_examples, test_metrics = centralized_flower_client.evaluate(\n",
    "    parameters=trained_params, config=test_config\n",
    ")\n",
    "print(loss, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXwkNqUED40K"
   },
   "source": [
    "# 6. FL with natural partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC0MPZAIM-JC"
   },
   "source": [
    "Given its naturally-partitioned nature, we can easily construct a realistic FL experiment by mapping clients one-to-one with the writers of the original symbols.\n",
    "\n",
    "To pursue this aim, we shall use the “naturally federated” partition instead of the \"centralised” one. We are then using the entire `train.csv` and `test.csv` contained in the subfolders of `client_data_mappings/fed_natural`. Each subfolder is named after the clients' ID in the dataset, i.e., from `0` to `3229`.\n",
    "\n",
    "To guarantee that each client has sufficient training data to participate meaningfully, a common practice is to set a lower bound on the number of samples a selected client is allowed to have. Generally, this threshold should be equivalent to at least one batch. We will now implement a function to sample clients from the federation that satisfies the abovementioned filter in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QQv15XxXhvJ"
   },
   "outputs": [],
   "source": [
    "def sample_random_clients(\n",
    "    total_clients: int,\n",
    "    filter_less: int,\n",
    "    partition: Path,\n",
    "    seed: int | None = Seeds.DEFAULT,\n",
    ") -> Sequence[int]:\n",
    "    \"\"\"Function to randomly sample clients.\n",
    "    A filter on the client train set size is performed.\n",
    "\n",
    "    Args:\n",
    "        total_clients (int): total number of clients to sample.\n",
    "        filter_less (int): max number of train samples for which the client is **discarded**.\n",
    "        partition (Path): path to the folder containing the partitioning.\n",
    "        seed (Optional[int], optional): seed for the random generator. Defaults to None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Sequence[int]: list of sample client ids as int.\n",
    "    \"\"\"\n",
    "    real_federated_cid_client_generator: Callable[\n",
    "        [int], FlowerRayClient\n",
    "    ] = get_flower_client_generator(network_generator, federated_partition)  # type: ignore\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    list_of_ids = []\n",
    "    while len(list_of_ids) < total_clients:\n",
    "        current_id = random.randint(0, 3229)\n",
    "        if (\n",
    "            real_federated_cid_client_generator(current_id).get_train_set_size()\n",
    "            > filter_less\n",
    "        ):\n",
    "            list_of_ids.append(current_id)\n",
    "    return list_of_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "togXum333D1L"
   },
   "source": [
    "While FEMNIST has more than 3000 clients, our small-scale experiments will not require more than 100 at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-s7oVYNXkwf"
   },
   "outputs": [],
   "source": [
    "total_clients: int = 100\n",
    "list_of_ids = sample_random_clients(\n",
    "    total_clients, centralized_train_config[\"batch_size\"], federated_partition\n",
    ")\n",
    "\n",
    "federated_client_generator: Callable[\n",
    "    [int], FlowerRayClient\n",
    "] = get_flower_client_generator(\n",
    "    network_generator, federated_partition, lambda seq_id: list_of_ids[seq_id]\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUrsR0JJwZ5R"
   },
   "source": [
    "Now, to test that the newly partitioned clients can be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HzR2ZY7nhPm"
   },
   "outputs": [],
   "source": [
    "one_epoch_config: Dict[str, Any] = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "test_config: Dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyoErIaM3D1L"
   },
   "outputs": [],
   "source": [
    "num_clients = 4\n",
    "clients = random.sample(list(range(total_clients)), num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FC1QOYivn_PX"
   },
   "outputs": [],
   "source": [
    "trained_models = [\n",
    "    fit_client_seeded(\n",
    "        federated_client_generator(cid), seed_model_params, one_epoch_config\n",
    "    )\n",
    "    for cid in clients\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATLQPQ_fm9ph"
   },
   "outputs": [],
   "source": [
    "trained_model_parameters = [model for model, *rest in trained_models]\n",
    "trained_model_metrics = [rest for _, *rest in trained_models]\n",
    "print(trained_model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgg0G0i23D1L"
   },
   "source": [
    "**Question 5 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "If we index the weights of an ML model consecutively, with the assumed architecture being the same, we can visualize it as a single flattened 1-D vector. One, highly imperfect, metric for determining how similar two ML models are is cosine similarity. We can use this metric to compare the models our clients produce are.\n",
    "\n",
    "1. Write functions to:\n",
    "    - Flatten the `NDArrays` objects into a single 1-D vector.\n",
    "    - Compute the cosine similarity of two 1-D vectors based on their inner product and norms.\n",
    "2. Compute a similarity matrix between all client models and plot it.\n",
    "3. What happens to the similarity matrix if you increase the number of local epochs that clients train for to 5 from 1? Why do you think that is?\n",
    ">You may want to save the parameters calculated here separately from the deault ones as both sets will be needed in future questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwWkQ6iixhDh"
   },
   "source": [
    "**Question 6 (Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "One reason that cosine similarity is an imperfect metric is because it considers weights from different layers to be equally important to the model in terms of its behavior and performance. We can thus create a slightly more detailed picture by tracking the cosine similarity across layers.\n",
    "\n",
    "1. Create a function which computes the pairwise cosine similarity between flattened model layers across clients, i.e. the cosine similarity between the flattened $l_i$ in `client_x` and the flattened $l_i$ in `client_y`.\n",
    "2. Plot the similarity of the models at each layer between the most similar client pair and the most disimilar client pair (as determined by the multi-epoch experiment) using the **one-epoch models**, how do the curves you observe relate to the general cosine similarity you computed for the flattened models.\n",
    "3. What happens to the curves if you increase the number of local epochs to 5? How does this behavior relate to what you observed in the previous exercise ?\n",
    "4. **Optional**: Do you think there is any connection to the underlying architecture? Why? Repeat the above experiments using an MLP such as the one provided by the  `get_network_generator_mlp` in `client_utils` and compare results against the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifI1FmOywjqL"
   },
   "source": [
    "# 7. Federated training with Flower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfDcWWoc2B5q"
   },
   "source": [
    "The two basic blocks of synchronous server-client FL systems are:\n",
    "- A client with some local training method and data---i.e., SGD. This is what we have built thus far.\n",
    "- A server which coordinates training sends the federated model to clients at the start of each round and aggregates model updates at the end of each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk9W7CTgoUxS"
   },
   "source": [
    "\n",
    "![picture](https://drive.google.com/uc?id=1Db_Uys2QPFHW6cMranZ_QXo2vC0A_C-N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdcfcz_e2B5q"
   },
   "source": [
    "While the variety of client local training methods is as wide as ML, server training varies depending on which **aggregation algorithm** combines the model updates and forms the new federated model. Flower refers to the component that controls the aggregation and train/test configuration as a **strategy**. For our labs, strategy and aggregation algorithms will be used interchangeably.\n",
    "\n",
    "The strategy we shall use is [FedAvg](https://arxiv.org/abs/1602.05629):\n",
    "\n",
    "$G^{r+1} = G^{r} + \\eta \\left( \\sum_{k=1}^{m} p_k G^r_k \\right)$,\n",
    "\n",
    "where $G^{r+1}$ is the model for the next round, which is formed by applying a model update to the current round model $G^{r}$ weighted by the aggregation learning rate $\\eta$. The model update is constructed by a sum of client models $G^{r}_k$ where each client model is weighted by $p_k$. The weight factor is usually set to $p_k = \\tfrac{n_k}{N}$---the number of training examples held by said client divided by the example count of each client in the round $N = \\sum_{k=1}^{m} n_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7unJaupF-IaB"
   },
   "source": [
    "**Question 7 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "Averaging models is meant to provide us with a reasonable compromise between the desired models of all clients involved. Given what you already know about the similarity of client models, we can test if this is true.\n",
    "1. Write a function to compute the weighted average of two `NDArrays` objects while preserving the layer-structure. Use the FedAvg equation above with $p_k = \\tfrac{n_k}{N}$.\n",
    "2. Compute the average model of the client models above, both the one-epoch and multi-epoch ones.\n",
    "3. Add the averaged models to the cosine-similarity matrices built above, how does it relate to the client models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHlT2igu-IaB"
   },
   "source": [
    "**Question 8 (Part III/MPhil ✅):**\n",
    "\n",
    "(You need to provide the answer with **code** for this question. Written argumentation is **strongly** recommended.)\n",
    "\n",
    "\n",
    "1. Repeat the procedure above but for layer-wise cosine similarity and plot the cosine similarity of the two clients client most similar and most disimillar to the averaged model (as determined by the five-epoch condition) using the one-epoch models.\n",
    "2. How does the relationship change for the higher-epoch condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxsFjUN03D1M"
   },
   "source": [
    "The pieces necessary for starting an FL simulation are now in play; we need to arrange them to fit the Flower API. First, we shall require a separate federated evaluation function which can be called outside the context of a specific client. It will use the centralised test set to be as simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO7CqzaE3D1M"
   },
   "outputs": [],
   "source": [
    "def get_federated_evaluation_function(\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    model_generator: Callable[[], Module],\n",
    "    criterion: Module,\n",
    "    max_batches: int,\n",
    ") -> Callable[[int, NDArrays, Dict[str, Any]], Tuple[float, Dict[str, Scalar]]]:\n",
    "    \"\"\"Wrapper function for the external federated evaluation function.\n",
    "    It provides the external federated evaluation function with some\n",
    "    parameters for the dataloader, the model generator function, and\n",
    "    the criterion used in the evaluation.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): batch size of the test set to use.\n",
    "        num_workers (int): correspond to `num_workers` param in the Dataloader object.\n",
    "        model_generator (Callable[[], Module]):  model generator function.\n",
    "        criterion (Module): PyTorch Module containing the criterion for evaluating the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Callable[[int, NDArrays, Dict[str, Any]], Tuple[float, Dict[str, Scalar]]]: external federated evaluation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def federated_evaluation_function(\n",
    "        server_round: int,\n",
    "        parameters: NDArrays,\n",
    "        fed_eval_config: Dict[\n",
    "            str, Any\n",
    "        ],  # mandatory argument, even if it's not being used\n",
    "    ) -> Tuple[float, Dict[str, Scalar]]:\n",
    "        \"\"\"Evaluation function external to the federation.\n",
    "        It uses the centralized val set for sake of simplicity.\n",
    "\n",
    "        Args:\n",
    "            server_round (int): current federated round.\n",
    "            parameters (NDArrays): current model parameters.\n",
    "            fed_eval_config (Dict[str, Any]): mandatory argument in Flower, can contain some configuration info\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tuple[float, Dict[str, Scalar]]: evaluation results\n",
    "        \"\"\"\n",
    "        device: str = get_device()\n",
    "        net: Module = set_model_parameters(model_generator(), parameters)\n",
    "        net.to(device)\n",
    "\n",
    "        full_file: Path = centralized_mapping\n",
    "        dataset: Dataset = load_FEMNIST_dataset(data_dir, full_file, \"val\")\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        loss, acc = test_FEMNIST(\n",
    "            net=net,\n",
    "            test_loader=valid_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            max_batches=max_batches,\n",
    "        )\n",
    "        return loss, {\"accuracy\": acc}\n",
    "\n",
    "    return federated_evaluation_function\n",
    "\n",
    "\n",
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    batch_size=test_config[\"batch_size\"],\n",
    "    num_workers=test_config[\"num_workers\"],\n",
    "    model_generator=network_generator,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    max_batches=test_config[\"max_batches\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D1cEkrB2B5r"
   },
   "outputs": [],
   "source": [
    "def aggregate_weighted_average(metrics: List[Tuple[int, dict]]) -> dict:\n",
    "    \"\"\"Generic function to combine results from multiple clients\n",
    "    following training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        metrics (List[Tuple[int, dict]]): collected clients metrics\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: result dictionary containing the aggregate of the metrics passed.\n",
    "    \"\"\"\n",
    "    average_dict: dict = defaultdict(list)\n",
    "    total_examples: int = 0\n",
    "    for num_examples, metrics_dict in metrics:\n",
    "        for key, val in metrics_dict.items():\n",
    "            if isinstance(val, numbers.Number):\n",
    "                average_dict[key].append((num_examples, val))  # type:ignore\n",
    "        total_examples += num_examples\n",
    "    return {\n",
    "        key: {\n",
    "            \"avg\": float(\n",
    "                sum([num_examples * metr for num_examples, metr in val])\n",
    "                / float(total_examples)\n",
    "            ),\n",
    "            \"all\": val,\n",
    "        }\n",
    "        for key, val in average_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sxyPBQ0zSoD"
   },
   "outputs": [],
   "source": [
    "# Federated configuration dictionary\n",
    "federated_train_config: Dict[str, Any] = {\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLIfqk0xhDj"
   },
   "source": [
    "The only challenge left is the FL simulation itself. In `Flower`, a `Server` object handles this for us by using `Ray` and spawning many heavyweight worker process.\n",
    "\n",
    "Given the limited-resource scenario in which we find ourselves, we provide you with a slightly modified simulation function which uses a simple thread pool. Feel free to swap it out for the original simulation or replace it with your own implementation if so inclined.\n",
    "\n",
    "> The server we use is not the default `Flower` server as it returns the model parameters from every single round in a `(round, NDArrays)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Fs5gtK-3D1M"
   },
   "outputs": [],
   "source": [
    "def save_history(hist: History, name: str):\n",
    "    time = int(datetime.now(timezone.utc).timestamp())\n",
    "    path = home_dir / \"histories\"\n",
    "    path.mkdir(exist_ok=True)\n",
    "    path = path / f\"hist_{time}_{name}.json\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hist.__dict__, f, ensure_ascii=False, indent=4, default=convert)\n",
    "\n",
    "\n",
    "def start_seeded_simulation(\n",
    "    client_fn: Callable[[int], Client],\n",
    "    num_clients: int,\n",
    "    config: ServerConfig,\n",
    "    strategy: Strategy,\n",
    "    name: str,\n",
    "    return_all_parameters: bool = False,\n",
    "    seed: int = Seeds.DEFAULT,\n",
    "    iteration: int = 0,\n",
    ") -> Tuple[List[Tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Wrapper to always seed client selection.\"\"\"\n",
    "    np.random.seed(seed ^ iteration)\n",
    "    torch.manual_seed(seed ^ iteration)\n",
    "    random.seed(seed ^ iteration)\n",
    "    parameter_list, hist = fl.simulation.start_simulation_no_ray(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        client_resources={},\n",
    "        config=config,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    save_history(hist, name)\n",
    "    return parameter_list, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkqplxMXLm8K"
   },
   "outputs": [],
   "source": [
    "num_rounds = 10\n",
    "\n",
    "num_total_clients = 20\n",
    "\n",
    "num_evaluate_clients = 0\n",
    "num_clients_per_round = 5\n",
    "\n",
    "\n",
    "def run_simulation(\n",
    "    # How long the FL process runs for:\n",
    "    num_rounds: int = num_rounds,\n",
    "    # Number of clients available\n",
    "    num_total_clients: int = num_total_clients,\n",
    "    # Number of clients used for train/eval\n",
    "    num_clients_per_round: int = num_clients_per_round,\n",
    "    num_evaluate_clients: int = num_evaluate_clients,\n",
    "    # If less clients are overall available stop FL\n",
    "    min_available_clients: int = num_total_clients,\n",
    "    # If less clients are available for fit/eval stop FL\n",
    "    min_fit_clients: int = num_clients_per_round,\n",
    "    min_evaluate_clients: int = num_evaluate_clients,\n",
    "    # Function to test the federated model performance\n",
    "    # external to a client instantiation\n",
    "    evaluate_fn=federated_evaluation_function,\n",
    "    # Functions to generate a config for client fit/evaluate\n",
    "    # by-default the same config is shallow-copied to all clients in Flower\n",
    "    # this version simply uses the configs defined above\n",
    "    on_fit_config_fn: Callable[\n",
    "        [int], Dict[str, Scalar]\n",
    "    ] = lambda cid: federated_train_config,\n",
    "    on_evaluate_config_fn: Callable[[int], Dict[str, Scalar]] = lambda cid: test_config,\n",
    "    # The \"Parameters\" type is merely a more packed version\n",
    "    # of numpy array lists, used internally by Flower\n",
    "    initial_parameters: Parameters = ndarrays_to_parameters(seed_model_params),\n",
    "    # If this is set to True, aggregation will work even if some clients fail\n",
    "    accept_failures: bool = False,\n",
    "    # How to combine the metrics dictionary returned by all clients for fit/eval\n",
    "    fit_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
    "    federated_client_generator: Callable[\n",
    "        [int], fl.client.NumPyClient\n",
    "    ] = federated_client_generator,\n",
    "    # Aggregation learning rate for FedAvg\n",
    "    server_learning_rate: float = 1.0,\n",
    "    server_momentum: float = 0.0,\n",
    ") -> Tuple[List[Tuple[int, NDArrays]], History]:\n",
    "    print(num_rounds)\n",
    "\n",
    "    # Percentage of clients used for train/eval\n",
    "    fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
    "    fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=fraction_fit,\n",
    "        fraction_evaluate=fraction_evaluate,\n",
    "        min_fit_clients=num_clients_per_round,\n",
    "        min_evaluate_clients=num_evaluate_clients,\n",
    "        min_available_clients=num_total_clients,\n",
    "        on_fit_config_fn=on_fit_config_fn,\n",
    "        on_evaluate_config_fn=on_evaluate_config_fn,\n",
    "        evaluate_fn=evaluate_fn,\n",
    "        initial_parameters=initial_parameters,\n",
    "        accept_failures=accept_failures,\n",
    "        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
    "        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "    )\n",
    "    # resetting the seed for the random selection of clients\n",
    "    # this way the list of clients trained is guaranteed to be always the same\n",
    "\n",
    "    cfg = ServerConfig(num_rounds)\n",
    "\n",
    "    simulator_client_generator = lambda cid: federated_client_generator(cid).to_client()\n",
    "\n",
    "    parameters_for_each_round, hist = start_seeded_simulation(\n",
    "        client_fn=simulator_client_generator,\n",
    "        num_clients=num_total_clients,\n",
    "        config=cfg,\n",
    "        strategy=strategy,\n",
    "        name=\"fedavg\",\n",
    "        return_all_parameters=True,\n",
    "        seed=Seeds.DEFAULT,\n",
    "    )\n",
    "    return parameters_for_each_round, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4HpswKXbDup"
   },
   "outputs": [],
   "source": [
    "parameters_for_each_round, hist = run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CMXI3Uz3D1M"
   },
   "outputs": [],
   "source": [
    "print(len(parameters_for_each_round))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mriFJbgc-IaC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY9ELp7NuXEE"
   },
   "source": [
    "**Question 9 (Part II ✅ | Part III/MPhil ✅):**\n",
    "\n",
    "(You must provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    "Now that an entire FL experiment can be trained, it is worth considering the relationship between the FL model across rounds. If we consider the final model $\\theta^N$ obtained after $N$ rounds to be the optimal model $\\theta^*$, we can ask how optimization step contributed to reaching $\\theta^*$.\n",
    "\n",
    "1. Run an FL simulation for at-least 50 rounds.\n",
    "2. Build a function which reconstitutes the update that must have been applied to model $\\theta^t$ in order to obtain $\\theta^{t+1}$, assume the `server_learning_rate` was 1.0. We shall call this update $g_t$ and treat it as a pseudo-gradient.\n",
    "3. For each round plot the cosine similarity between $g_t$ and the direction of improvement towards $\\theta^{*}$ calculated as $\\theta^{*} - \\theta_t$.\n",
    "4. What do you observe from the plots? Does every update point in the direction of the final model? If not, why do you think that is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6ofvhZqDSnA"
   },
   "source": [
    "**Question 10 (Part III/MPhil ✅):**\n",
    "\n",
    "(You must provide the answer with **code** and **plots** for this question. A short written argumentation is recommended.)\n",
    "\n",
    " and then for\n",
    "\n",
    "1.   Read about server momentum in FL [here](https://arxiv.org/abs/2003.00295) and repeat the previous experiments using `server_learning_rate=1.0` and `server_momentum=0.9`. Discuss how the momentum impacts the direction of optimization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WfWeMqLQ0oT"
   },
   "source": [
    "(c) 2024 Alexandru-Andrei Iacob, Lorenzo Sani"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "224d0ab12de28608c5a4d7a39646ed9b98595177ad290f2912bfa68781d77d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
